{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region selected  for the project\n",
    "I decided to work with the map of the city of Zurich, Switzerland. For private and professional reasons, I will be relocating to Zurich and I figured it might be a good idea to get acquainted with the city I will be living in in the near future.\n",
    "For this purpose an OSM file in XML format, representing the city of Zurich, was downloaded from http://www.openstreetmap.org."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get an overview about the OSM data \n",
    "After downloading the OSM file, I created a smaller OSM subset (zurich_sample.osm) that contains the main elements (node, way, relation). This file was manually pre-screened to get an idea how particular information is stored. Moreover, the smaller variant of the OSM file was used to check if any code (e.g. code for auditing) is working, before applying the code together with the original-size OSM file.\n",
    "Information on the general structure of the OSM XML file format can be found on http://wiki.openstreetmap.org/wiki. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "osm_file = \"zurich.osm\"\n",
    "def get_element(osm_file, elements = (\"node\", \"way\", \"relation\")):\n",
    "    treebuilder = ET.iterparse(osm_file, events = (\"start\",\"end\"))\n",
    "    # skip the the root element and the next 3 elements (bounds, meta, node)\n",
    "    for i in range(3):\n",
    "        _, skip = next(treebuilder)\n",
    "    for event, element in treebuilder:\n",
    "        if event == \"end\" and element.tag in elements:\n",
    "            yield element\n",
    "            skip.clear()\n",
    "            \n",
    "           \n",
    "with open(\"zurich_sample.osm\", \"w\") as file_out:\n",
    "    file_out.write(\"<?xml version='1.0' encoding='UTF-8'?>\\n\")\n",
    "    file_out.write(\"<osm version='0.6' generator='Overpass API'>\\n\")\n",
    "    file_out.write(\"<note>The data included in this document is from www.openstreetmap.org. The data is made available under ODbL.</note>\\n\")\n",
    "    file_out.write(\"<meta osm_base='2017-05-15T16:51:17Z'/>\\n\")\n",
    "    file_out.write(\"<bounds minlat='47.2734000' maxlon='8.8031000' minlon='8.2700000' maxlat='47.4815000'/>\\n\")\n",
    "    \n",
    "    for i, element in enumerate(get_element(osm_file)):\n",
    "        if i%4000 == 0:\n",
    "            file_out.write(ET.tostring(element, encoding='utf-8'))\n",
    "            \n",
    "    file_out.write(\"</osm>\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to audit?\n",
    "I selected following element attributes of the OSM file for auditing and cleaning:\n",
    "1. id, uid and version (audit_id_version.py)\n",
    "2. lat and lon (audit_coordinates.py)\n",
    "3. timestamp (audit_timestamp.py)\n",
    "4. addr:street (audit_street.py)\n",
    "5. addr:housenumber (audit_housenumber.py)\n",
    "6. addr:city (audit_city.py, crossaudit_city_postcode.py)\n",
    "7. addr:postcode (audit_postcodes.py, crossaudit_city_postcode.py)\n",
    "\n",
    "\n",
    "Scripts used for auditing are given in parentheses. For more information about the scripts used for data auditing, cleaning and processing, please refer to the readme file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audit element id, uid and version\n",
    "Id's and user Id's are either stored as attributes \"id\" and \"uid\" in all primary elements or they are stored as the attribute \"ref\" in the second level elements nd (child of first level element way) and member (child of first level element relation). The attribute \"version\" is stored in all first level elements.\n",
    "Valid \"id\", \"uid\" and \"version\" attribute values must be convertible to integer type. \n",
    "\n",
    "### Results\n",
    "no irregularities found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audit geographical coordinates (latitude and longitude)\n",
    "The attributes \"lat\" and \"lon\" represent geographical coordinates and are stored in the first level element node. Valid coordinates must be convertible to float type but not integer type\n",
    "\n",
    "### Results\n",
    "no irregularities found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audit timestamp \n",
    "The attribute timestamp refers to a representation of time and date and is stored in all three first level elements.\n",
    "According to the International Standard for the representation of dates and times, valid time formats are (which can be querried with SQLite):\n",
    "1. YYYY-MM-DD hh:mm:ss\n",
    "2. YYYY-MM-DDThh:mm:ssZ\n",
    "\n",
    "Y: Year, M: Month, D: Day, h: Hour, m: Minute, s: second\n",
    "T seperated date and time and Z designates UTC (Coordinated Universal Time) time\n",
    "\n",
    "- please refer to https://www.w3.org/TR/NOTE-datetime for more information \n",
    "\n",
    "### Results\n",
    "no irregularities found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Audit street names\n",
    "\n",
    "Street name attributes are stored as key-value pairs with key = \"addr:street\" tagging the attribute as an information container for a street name and value = \"any name\" defining the actual name of the street. \"addr:street\" is stored in second level elements tag.  \n",
    "\n",
    "Unfortunately there is no concise terminology or convention for naming streets in german-speaking countries (e.g. Switzerland). Sometimes they can be written in two words, where one Noun is joined with a \"typical\" street category (Strasse -street-, Platz -square-, Allee -avenue-, Weg -way-), as in \"Mainzer Strasse\", which implies the meaning \"in honor of\" (street named in honor of the City Mainz). However, two words can be combined to name a street, without any of the words describing a typical street category (e.g, \"Oberere Weid\", \"Hinter Zünen\") or even more than two words can be used to name a street (e.g, \"An der Halde\"). In both cases -without street category extension- the name simply has an inherent geographic description for that particular street. E.g, \"Oberere Weid\" means that the street is located next to a high-altitude meadow or atop a mountain/hill, as in \"Am Berg\". As if this isn't complicated enough, basically the same street name can be written in multiple variations depending on regional specifications. E.g, \"Obere Weid\", \"Ober Weid\", \"Obere Weide\", \"Obere Weiden\" are all valid street names in Switzerland/Germany. So with respect to the latter example, you can't be sure that \"Weid\"/\"Weiden\" is a misspelling of \"Weide\"!\n",
    "Additionally, street names can consist of one-word trivial names that are inherently ambiguous. E.g., \"Vorstadt\" is a valid street name in Zurich but the word literally translates to \"suburb\". Moreover, each of the above mentioned two-word variants are existent as one-word combinations, such as \"Holzweid\" or \"Müssmattstrasse\". In fact, one-word street names are the most common variant!!  \n",
    "\n",
    "As such, the most significant issue in terms of identifying any wrong spelling or abbreviations  in street names, is the lack of standardized naming pattern, e.g with street types as distinct words separated from the rest of the name, which is the standard for UK/ american street names (e.g \"7th avenue\"). On the one hand, this fact makes it necessary to use basic regular expression checks to screen for misspelling or abbreviations for street types, which on the other hand, impedes extraction of different types of abbreviations or misspelling, as for instance using \"st\" as a regex check to identify \"-str\"/\"-strrasse\"/\"-stasse\", without mostly and wrongly extracting valid street names, such as \"Kirchenstiege\" (by using st in a regular expression check)!! This is even more of an issue for street types having, for instance \"Allee\" or \"Weg\"  in their names, as the combination of vowel-consonant/consonant-vowel, if using \"al or we\" as a regular expression check, is vastly present in valid street names (\"Waldgass\", \"Waltikon\" and many many more would be returned as invalid). The bigger the dataset, the more apparently invalid street names will be returned, which at some point makes automated auditing obsolete!!\n",
    "\n",
    "\n",
    "### Strategy\n",
    "So I developed following strategy to identify most of invalid street name entries without returning too many false negatives and, thus, making the analysis very cumbersome.\n",
    "\n",
    "1. For each of the expected street types, check for word-character insertions (e.g, insertion of \"a\" in \"Plaatz\"). Including insertions of characters not present by default in the name (e.g \"e\" in \"Pleatz\") would require a much more sophisticated approach, which I am not yet capable of implementing.\n",
    "\n",
    "2. Check for abbreviations or deletion.  \n",
    "-For each of the names in the expected street types set, use the first two characters to define a regular expression  (e.g, \"st\" to find Müssmattstr. or Müssmattstrase).  \n",
    "-allow 3 additional word-characters to follow the two-letter regular expression for all expected street types but \"strasse\". For \"strasse\" allow up to 4 additional word-characters (simply because \"strasse\" is the string with most leters).\n",
    "-restrict the regular expression check to search for a match at the end of a string\n",
    "\n",
    "I make the assumption that insertions include only characters from which the street type name is constructed of (e.g \"p,l,a,t,z\" in \"Plaatz\" with \"a\" insertion), simply by pressing a key twice by mistake. Separating auditing for misspelling by insertion and deletion/abbreviation,  allows for a more stringent search of deletions, because I can reduce the number of word-characters following the two-letter regular expression. As such, I assume to overall minimize the number of false positives (e.g regex check using \"st\" would return \"Rostlaubenweg\" -valid name-, if more than 4 additional word-characters are allowed in the expression) without missing too many true positives (regex matching misspelling of street type).\n",
    "\n",
    "\n",
    "\n",
    "### Results\n",
    "\n",
    "**misspelling by insertion** as in \"Römerstrassse\" (\"s\" insertion in strasse)\n",
    "\n",
    "**misspelling by deletion** as in \"Hofwiesenstrsse\" (\"a\" deletion in strasse)\n",
    "\n",
    "**use of dialect** as in \"Im Chlösterli\" or \"Gass\" (in official language \"Im Kloster\" or \"Gasse\")\n",
    "this is not wrong per se, however, if including validation of street names against a reference database, it might be a problem. I will go into more detail in the section  \"Strategy for correcting street names, city name and postcodes\"\n",
    "\n",
    "**abbreviations** as in \"Brunnackerstr\" (strasse instead of str)\n",
    "\n",
    "**digits in street names** as in \"zuercherstrasse 95\" or \"Schiffbaustrasse 9b\" (housenumber as part of the name)\n",
    "\n",
    "**name consisting of digit only** (e.g, \"6\")\n",
    "\n",
    "**abbreviations and digits in street name** as in \"Alte Bahnhofstr. 22\"\n",
    "\n",
    "The follow up data cleaning of street names includes correction of insertion, deletion and abbreviations in street types into the expected naming pattern. Digits will be removed from street names and street names consisting of digits only, will be excluded in the process of setting up the database. For this purpose, the function street_clean() was used (see osm_cleaning.py script). For more details, please refer to the section \"Strategy for correcting street names, city name and postcodes\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audit housenumber\n",
    "Housenumbers are also stored in second level elements tag as key-value pairs with key = \"addr:housenumber\".\n",
    "\n",
    "### results first iteration\n",
    "Initially, I defined valid housenumbers to consist of digits only, or digits followed by one word character. The respective audit function returned all values that don't match a valid housenumber (via regular expression check).  \n",
    "This first check returned many entries consisting of multiple housenumbers (separated by different non-word characters; e.g 53/54 and optionally including word-characters; e.g 9a-9c) stored as one value. However, based on openstreetmap wiki, multiple housenumbers are expected to occur. \n",
    "\n",
    "### results second iteration\n",
    "As such, I conducted a second iteration using a different regular expression that matches single or multiple housenumbers, possibly extended by word characters (e.g, 9, 9a, 9 a, 9-9, 9-9b, 9a-9b). In this approach, I assign all values that **do not match** the regular expression as being an invalid housenumber!\n",
    "Following invalid entries were returned after the second iteration step: only word characters (e.g \"A\"), word character followed by digits (A4812), street name with/without housenumber (e.g \"Im Chies 14\", \"144 Im Hof\", \"Sportschützen Uster\").\n",
    "\n",
    "Of all the invalid housenumber values, only those having the street name and the housenumber can be corrected by simply stripping the street name. The other invalid values will be will be excluded in the process of setting up the database. A4812 has the format of swiss highways (A followed by a number), however the number is too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audit City\n",
    "Similar to street names, the city name attribute is stored in the second level element tag as a key-value pair with key = \"addr:city\".\n",
    "\n",
    "I expected all city names (values for the attribute \"addr:city\" in second level \"tag\" element) being confined to \"Zurich\". However, I wanted to get an overview if different spelling variants for \"Zurich\" were used. This is very common for german words that have muted vowels (Umlaut), such as \"ä,ü,ö\". Expected variants include \"Zürich\" (muted vowel  \"ü)\", \"Zuerich\" (ue substituted for muted vowel ü) and \"Zurich\" (common variant in english, which simply drops the diacritical marks).  \n",
    "However, after having a sneak peak at the smaller Zurich OSM sample, I noticed for some second level tag elements to have city names other than Zurich (e.g \"Dielsdorf\"). Therefore, I used a function to return a set for all Zurich variants, as well as a set for all other names. \n",
    "\n",
    "\n",
    "### Results first iteration\n",
    "All of the aforementioned spelling variants of Zurich were found. Additionally, entries with the city name of Zurich in combination with a city district were present (e.g \"Zurich-Altstetten\"). Additionally, names entirely composed of digits (e.g 8002 and 50; likely postcodes and housenumbers) were also present. However, much more cities other than Zurich were returned, some of which having abbreviations (e.g \"Affoltern a.A.\") or two-letter extensions (e.g. \"Aesch (ZH))\" as part of the name.  \n",
    "It seems that the Zurich city OSM file includes entries that actually describe the Canton of Zurich. Cantons are the member states of the country of Switzerland and the Canton of Zurich comprises of 12 districts to which the city of Zurich as well as the aforementioned Dieseldorf belong.  \n",
    "Instead of screening the entire long list returned by audit_city() function, I decided to include a second iteration and systematically check for entries containing particular non-word characters, as these seem to be present in city names that have abbreviations and word extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results second iteration\n",
    "More abbreviation variants were found (e.g \"Oberwil b. Nürensdorf\", \"Wettswil a. Albis\"). Additionally, two-letter extensions in different variations (e.g \"Buchs (ZH)\", \"Buchs ZH\", \"Winterberg/ZH\"), as well as word extensions in different variations (e.g \"Riedikon (Uster)\", \"Forch (Aesch, Maur)\", \"Aathal-Seegräben\", \"Grafstal/Kempttal\") were found. Furthermore, 4 cities with state extension other than state (Canton) of Zurich  were found, e.g \"Muri (AG)\", a city located in the state of Aargau. Finally, different spelling styles for the same city name were found (e.g \"Aathal - Seegräben\" and \"Aathal-Seegräben\", \"Uitikon-Waldegg\" and \"Uitikon Waldegg\")\n",
    "\n",
    "Extensions to city names either provide the affiliation of a village/city to a certain state in switzerland (in case of two-letter extensions) or to a certain municipality (e.g \"Oberwil\"; city \"Oberwil\" in municipality \"Nürensdorf\" versus municipality \"Cham\" ) or to both, municipality and state (\"Oberwil\" in municipality \"Nürensdorf\" in state \"Zurich\" versus \"Oberwil\" in municipality \"Cham\" in state \"Zug\").  \n",
    "However, it is very difficult to check for valid naming pattern for names with word extensions without having some kind of reference. The reason is that for most names, it is unclear whether the first or the second word describes the city name or the municipality name, or maybe neither of both, or if the same city name with and without an extension actually refers to the same city or to different cities located in different municipalities/states (e.g \"Ottikon\" vs. \"Ottikon\" (Gossau)\"). As such, a cleaning strategy that for example would only consider the first word in a name as the city name, would return many false positives. \n",
    "\n",
    "So I decided to correct abbreviations, different spelling styles for the same city name, digits as name, and correct two-letter extensions to have one consistent form. City names with state affiliation other than state of Zurich will be excluded. Knowledge about the meaning of abbreviations is based on experience in german language or from cognate city names without abbreviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audit postcode\n",
    "Postcodes are stored in second level elements tag as key-value pairs with key = \"addr:postcode\".\n",
    "\n",
    "Valid Zurich city postcodes are supposed to be 4-digit numbers starting with 8 and being in the range 8001-8064. Based on the results from auditing city names (city names other than Zurich city), I expected to find postcodes that are not conform with specifications for valid Zurich city postcodes. \n",
    "\n",
    "### Results\n",
    "Accordingly, 4-digit Postcodes starting with 3,4,5,6 and 9 were found, as well as postcodes consisting of word characters (e.g \"q\") were found. I decided to exclude those postcodes consisting of word-characters only and to keep all other postcodes \n",
    "\n",
    "Expected Postcodes were obtained from http://www.plz-suche.org/zuerich-ch7e45 and can be found in audit_postcodes.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Audit postcode/City\n",
    "Bearing in mind that unexpected city names and postcodes are present in the OSM file, I furthermore assumed wrong combinations of city name and postcode, either unexpected postcodes referring to the city of Zurich or unexpected city name referring to expected postcodes for the city of Zurich.  \n",
    "\n",
    "### Results\n",
    "Quite a lot of invalid city name-postcode combinations were found. E.g, city name \"Baden\" refering to 8 different postcodes, all of which are expected for the city of Zurich, or unexpected postcode 5400 referring to the city of Zurich. Additionally, postcodes starting with 8 but not belonging to the expected set of Zurich postcodes (e.g, postcode 8330 in combination with Zurich as the city)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy for reviewing the validity of street names, city name and postcodes\n",
    "To me, it was very surprising to find so many wrong city names and postcodes present in the OSM file that is supposed to contain information exclusively about the city of Zurich. As already noticed, many of those wrong entries simply refer to the state of Zurich instead of being restricted to the city of Zurich! Based on this explicit mistakes, I strongly assume that many street names are included that actually don't belong to the street set of the city of Zurich!! For first level elements (node, way, relation) having tags specifying street names, city names and postcodes, respectively, it is basically unclear which entries are actually right or wrong in terms of validity (just the postcode, or both postcode and city, or is the street located outside the city boundaries of Zurich city?). \n",
    "\n",
    "Because the OSM file contains thousands of wrong entries, I decided to set up a \"gold standard\" with valid street names and corresponding postcodes for the city of Zurich, which can be used to audit validity of OSM address tags. \n",
    "\n",
    "### Define the gold standard to validate address specifications in OSM file\n",
    "The goal was to create a reference dataset that contains all street names and the corresponding postcodes. Since Zurich is divided in 11 districts (e.g \"Kreis-7\"), each in turn consisting of different quarters (e.g quarters \"Hottingen\" and \"Witikon\" are located in district \"Kreis-7\"), I decided to include information about the district and quarter affiliation in the reference dataset. So, the final csv files and SQL database, respectively, will be extended by address information on quarter and district affiliation.  \n",
    "\n",
    "I used http://www.streetdir.ch/CH/Zurich/Zurich/Strassenverzeichnis to mine data containing the relevant information. The browser developer tool was used to determine the valid method for making successful HTTP requests (GET method) and to screen how the relevant data (street name, quarter, district, postcode) is stored in the HTML documents. \n",
    "\n",
    "The above mentioned URL retrieves the html document that contains all streetnames with initial letter A-F. By extending the URL with query parameters \"G\", \"L\" or \"S\", you will retrieve the html documents containing street names with initial letter G-K, L-R and S-Z, respectively. Each street is stored as a hyperlink, which will be used to eventually fetch the street name, quarter and district affiliation and postcode.\n",
    "**The function query_site() will be used to make HTTP requests (see code in section below)**\n",
    "\n",
    "In the cognate HTML files, relevant street information is stored in hyperlink tags \"a\" within \"table\" tags. Each hyperlink tag contains a \"class\" and \"href\" attribute. The \"class\" attribute value (\"DirectoryStreetLink\") can be used to select (via BeautifulSoup) all street names and relevant information (\"href\" value), respectively, to retrieve cognate quarter, districts and postcodes.\n",
    "**The function get_street_information() makes use of the query_site() function to make all HTTP requests and to save the data as csv file (see code in section below)** \n",
    "\n",
    "### Rules for auditing validity and making updates\n",
    "**If OSM street name is present in reference dataset**\n",
    "1. if city name is Zurich, update postcode, district and quarter according to reference data for the cognate street name\n",
    "\n",
    "2. if postcode is expected for Zurich, update city name, district and quarter according to reference data for the cognate street name\n",
    "\n",
    "3. if first level element has only a street tag (no tag for city, postcode), create tags for city name, postcode, district and quarter according to reference data for the cognate street name.\n",
    "\n",
    "matching either postcode or city name together with street name is required to discriminate same street names belonging to different cities \n",
    "\n",
    "**If OSM street name is not present in reference dataset**\n",
    "1. if city tag value is Zurich, update value to \"Zurich municipality\"\n",
    "\n",
    "2. if first level element has no city tag, create city tag with \"Zurich municipality\" as value\n",
    "\n",
    "3. if city tag value is not Zurich, keep city name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import codecs\n",
    "\n",
    "\n",
    "\n",
    "# from Class\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "url_BASIC = \"http://www.streetdir.ch\"\n",
    "url_STREETDIRECTORY = url_BASIC + \"/CH/Zurich/Zurich/Strassenverzeichnis\"\n",
    "params = (None, \"G\", \"L\", \"S\")\n",
    "\n",
    "\n",
    "def query_site(url, params):\n",
    "    '''\n",
    "    main function to make queries\n",
    "         \n",
    "    params:  query parameter to retrive the correct html file (streetnames with particular starting word characters).\n",
    "             G, L, S, None are valid params (If params = None provided, query will be performed without a query\n",
    "             parameter, which will return streetnames A-F). \n",
    "             G = street names G-K; L = street names L-R; S = street names S-Z\n",
    "    url: url string         \n",
    "    \n",
    "    '''\n",
    "    params_dict = {\"char\" : params}\n",
    "    r = requests.get(url,params_dict)\n",
    "    return r.text\n",
    "\n",
    "def get_street_information(url_BASIC,url_STREETDIRECTORY, params):\n",
    "    '''\n",
    "    uses the query_site function to fetch names, zipcodes, district and quarter affiliation for all streets in zurich.\n",
    "    \n",
    "    url_STREETDIRECTORY (http://www.streetdir.ch/CH/Zurich/Zurich/Strassenverzeichnis) -can be extended by query \n",
    "    parameter \"char\"- lists all street names for the city of Zurich. For each street a relative URL path will be \n",
    "    extracted that is joined with url_BASIC and provides a new absolute URL, which is used to scrape (via query_site()\n",
    "    and BeautifulSoup) the name, quarter and district affiliation and postcode.\n",
    "    \n",
    "    \n",
    "    Data will be written as csv file \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    with codecs.open(\"street_names_zipcodes_zurich\", \"w\") as file_out:\n",
    "        fieldnames = [\"district\", \"quarter\", \"street\", \"zipcode\"]\n",
    "        writer = UnicodeDictWriter(file_out, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "    \n",
    "        for query_param in params:\n",
    "            data = query_site(url_STREETDIRECTORY, query_param)\n",
    "            soup = BeautifulSoup(data,\"lxml\")\n",
    "    \n",
    "            for item in soup.findAll(\"a\", attrs={'class':'DirectoryStreetLink'}):\n",
    "                street_PATH = item.get(\"href\")\n",
    "                data = query_site(url_BASIC + street_PATH, None)\n",
    "                soup_within = BeautifulSoup(data,\"lxml\")\n",
    "                \n",
    "                street = soup_within.find(\"span\", attrs={\"id\":\"yah5\"}).text\n",
    "                quarter = soup_within.find(\"span\", attrs={\"id\":\"yah4\"}).text\n",
    "                zipcode = soup_within.find(\"meta\", attrs={\"name\":\"keywords\"})[\"content\"].split(\",\")[-1]\n",
    "                district = soup_within.find(\"span\", attrs={\"id\":\"yah3\"}).text\n",
    "                \n",
    "                writer.writerow({\"district\":district, \"quarter\":quarter, \"street\":street, \"zipcode\":zipcode})\n",
    "                \n",
    "        \n",
    "    \n",
    "    \n",
    "get_street_information(url_BASIC,url_STREETDIRECTORY,params)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audit the reference dataset\n",
    "Similar to the OSM data, the reference dataset requires a screening to identify potentially invalid entries. \n",
    "\n",
    "### First iteration\n",
    "Following results were recorded. Districts are either specified by numbers (e.g, \"Kreis-9\"), or by trivial names (e.g \"Wiedikon\"). Both are correct, but for the purpose of consistency trivial names will be substituted for the corresponding numbered variants (e.g \"Wiedikon\" substituted for \"Kreis-3\"; data obtained from https://en.wikipedia.org/wiki/Subdivisions_of_Z%C3%BCrich). The code below was used to update districts\n",
    "Some of the postcodes are not valid postcodes for the city of Zurich (e.g, 8122, 8142) and some postcodes have a wrong format by starting with 0 (e.g, 08049). No irregularities in street names were found. \n",
    "Based on the irregularities found, I suspect invalid combinations of postcodes and quarters/districts or quarters and districts. \n",
    "\n",
    "auditing was conducted by using audit_reference_data() function present in audit_reference.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping_districts = {\"Industriequartier\" : \"Kreis 5\",\n",
    "                     \"Wiedikon\" : \"Kreis 3\",\n",
    "                     \"Schwamendingen\" : \"Kreis 12\",\n",
    "                     \"Aussersihl\" : \"Kreis 4\",\n",
    "                     \"Riesbach\" : \"Kreis 8\",\n",
    "                     \"Altstadt\" : \"Kreis 1\"}\n",
    "\n",
    "def update_districts(entry):\n",
    "    '''\n",
    "    use mapping_districts to update districts\n",
    "    '''\n",
    "    if entry in mapping_districts:\n",
    "        return mapping_districts[entry]\n",
    "    else:\n",
    "        return entry\n",
    "    \n",
    "ref_data = pd.read_csv(\"street_names_zipcodes_zurich\", dtype=str)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**update district naming** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ref_data[\"district\"] = ref_data[\"district\"].apply(update_districts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second iteration\n",
    "Consequently, I defined two sets of expected combinations of districts and quarters, as well as quarters and postcodes (data obtained from http://www.plz-suche.org/zuerich-ch7e45, https://en.wikipedia.org/wiki/Subdivisions_of_Z%C3%BCrich; sets can be found in audit_reference.py). After updating district names to numbered versions, no invalid combinations between quarters and districts were found. However, quarters with wrong postcodes were found -additionally to the expected invalid combinations for postcodes starting with 0 and those postcodes invalid for the city of Zurich- (e.g, quarter \"Witikon\" with postcode 8032, which is the postcode for quarter \"Hirslanden\"). \n",
    "Following code was used to correct postcodes. An updated version of the reference dataset (\"street_names_zipcodes_zurich_update\") was generated, which will be used as the \"gold standard\" to validate address specifications of the zurich OSM file.\n",
    "\n",
    "auditing was conducted by using audit_district_quarter_crossref() and audit_quarter_postcode_crossref() functions present in audit_reference.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**update postcodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from audit_reference import*\n",
    "\n",
    "def update_postcodes(entry):\n",
    "    '''\n",
    "    use expected_quarters_postcodes dictionary from audit_reference.py to correct postcodes\n",
    "    '''\n",
    "    return expected_quarters_postcodes[entry[\"quarter\"]]   \n",
    "\n",
    "\n",
    "ref_data[\"zipcode\"] = ref_data.apply(update_postcodes, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third iteration\n",
    "I realized that I need to check whether the street names in the reference dataset are unique. This is important to adequately adapt the implementation of validating and updating the Zurich OSM file by using the reference dataset (see section \"Updated rules for auditing validity\" below). \n",
    "Indeed, some street names were present multiple times in the reference dataset (e.g, 2x \"Flurstrasse\"). I included \"district\", \"street\" and \"postcode\" information to verify only duplicates having the same \"postcode\" and \"district\". These will be removed from the reference dataset, whereas duplicate street names from different districts will remain in the reference dataset, as for those no further -automized- validation is possible and same street names for different districts exist, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remove street name duplicates and write updated reference dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ref_data = ref_data.drop(ref_data[ref_data[[\"district\",\"street\",\"zipcode\"]].duplicated()].index.values)\n",
    "ref_data.to_csv(\"street_names_zipcodes_zurich_update\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated rules for auditing validity and making updates\n",
    "**If OSM street name is present multiple times in reference dataset**\n",
    "1. If the cognate OSM postcode (from same first level element) has a match in the reference dataset that corresponds to the street name, update city, district and quarter according to reference data for the cognate street name\n",
    "\n",
    "No updates If the cognate OSM postcode does not match the reference dataset postcode that corresponds to the OSM street name, or if no OSM postcode tag is present.\n",
    "\n",
    "## Ultimate procedure for cleaning street names, city name and postcodes\n",
    "First stage of cleaning includes the correction of basic irregularities, e.g correction of abbreviations, misspelling in street names, removal of no-digit postcodes or consistent spelling for Zurich, etc. The relevant functions for first stage cleaning are included in osm_cleaning.py.\n",
    "Following first stage, the second stage will include the revised reference dataset (\"street_names_zipcodes_zurich_update\") for validation of relevant values (street name, city name, postcode) and making appropriate updates (include district and quarter affiliation). Validation and update rules are described in the sections \"updated -/ rules for auditing validity and making updates\" in more detail. data.py implements both cleaning steps and eventually creates csv files containing the relevant and cleand data points from the Zurich OSM file (for more information refer to data.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the database\n",
    "The SQL database (zurichOSM.db) was created via command line using sqlite3. For each csv file returned by data.py, a database table was created with standard SQL table schema (for more information, please refer to www.sqlite.org) using following command. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sqlite> CREATE TABLE tablename(column1 TYPE [constraint], column2 TYPE [constraint],...);  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If appropriate, PRIMARY KEY or FOREIGN KEY constraints were specified. After creation of a table, following command was used to import the corresponding csv file into the table."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sqlite> .mode csv  \n",
    "sqlite> .import filename.csv tablename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware that for csv files with header, the header will be imported into the table when using sqlite3 command line!!\n",
    "Delete header row from table. E.g, following command will delete the row with id as value for the column id from relations table. Header rows have been deleted for all tables in the zurichOSM.db."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "DELETE FROM relations WHERE id = 'id';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all queries conducted via command line using sqlite3\n",
    "### Number of nodes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SELECT count(*) FROM nodes; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1993633**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of ways"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SELECT count(*) FROM ways;  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**314124**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of relations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SELECT count(*) FROM nodes;  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5046**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique users"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SELECT count(uni.uid) \n",
    "FROM(SELECT uid FROM nodes UNION SELECT uid from ways UNION SELECT uid from relations) as uni;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2356**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earliest and latest contribution"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SELECT min(entry.timestamp) \n",
    "FROM (SELECT timestamp FROM nodes UNION SELECT timestamp FROM ways UNION SELECT timestamp FROM relations) as entry;\n",
    "\n",
    "SELECT max(entry.timestamp) \n",
    "FROM (SELECT timestamp FROM nodes UNION SELECT timestamp FROM ways UNION SELECT timestamp FROM relations) as entry;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2006-05-05** and **2017-05-15**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min, max and average contributions per year and total contributions for top 10 users"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CREATE VIEW user_contribution as SELECT user, min(num) as min_contr, max(num) as max_contr,round(avg(num),1) as avg_contr, sum(num) as total_contr\n",
    "FROM\n",
    "(SELECT entry.user,substr(entry.timestamp,1,4),count(*) as num \n",
    "FROM (SELECT user,timestamp FROM nodes UNION ALL SELECT user,timestamp FROM ways UNION ALL SELECT user,timestamp FROM relations) as entry\n",
    "GROUP BY entry.user, substr(entry.timestamp,1,4))\n",
    "GROUP BY user\n",
    "ORDER BY sum(num); \n",
    "\n",
    "SELECT * FROM user_contribution\n",
    "ORDER BY total_contr DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "user                 min    max      average   total\n",
    "\n",
    "SimonPoole           9810   134267   40089.4   320715  \n",
    "mdk                  726    57688    25946.8   233521  \n",
    "Sarob                104    100801   29153.4   145767  \n",
    "feuerstein           32     82699    20465.8   102329  \n",
    "hecktor              29     47097    12771.9   89403  \n",
    "joshx                40     28569    11007.1   77050  \n",
    "Hami                 1      58503    7996.9    63975  \n",
    "ueliw0               15     38550    9024.4    63171  \n",
    "ponte1112            18     49203    8415.3    58907  \n",
    "captain_slow         4844   23003    13494.5   53978"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contribution of top 1%, 5% and 10% of users\n",
    "24, 118 and 236 users with highest total contributions depict the top 1-,5- and 10% of users, respectively. Use this numbers for the LIMIT command in the code below to obtain the respective perentages."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SELECT sum(percentage) FROM \n",
    "(SELECT user,ROUND( CAST((total_contr*100) as REAL)/(SELECT sum(total_contr) FROM user_contribution),2) as percentage\n",
    "FROM user_contribution group by user\n",
    "ORDER by percentage DESC\n",
    "LIMIT 24\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 1%** made 72.55% of total contributions  \n",
    "**Top 5%** made 93.43% of total contributions  \n",
    "**Top 10%** made 96.93% of total contributions  \n",
    "\n",
    "Only 5% of total users made almost 95% of the total dataset (contribution reaches a plateau from including the top 5%) !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of restaurants, cafes, bars, pubs and top 20 cuisines"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SELECT value,count(*) \n",
    "FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags UNION ALL SELECT * from relations_tags)\n",
    "WHERE key = 'amenity' \n",
    "AND value IN ('restaurant', 'bar','cafe','pub');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2577** places serving food and beverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of restaurants, cafes, bars, pubs and top 20 cuisines in Zürich"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SELECT a.key,b.key, a.value, b.value, count(*) \n",
    "FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags UNION ALL SELECT * from relations_tags) as a \n",
    "JOIN \n",
    "(SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags UNION ALL SELECT * from relations_tags) as b\n",
    "ON a.id = b.id\n",
    "WHERE a.key = 'amenity'\n",
    "AND a.value IN ('restaurant', 'bar','cafe','pub')\n",
    "AND b.key = 'city'\n",
    "AND b.value = 'Zürich';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**736** localities in Zürich serving food and beverage"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SELECT a.key,b.key, a.value, b.value, count(*) \n",
    "FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags UNION ALL SELECT * from relations_tags) as a \n",
    "JOIN \n",
    "(SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags UNION ALL SELECT * from relations_tags) as b\n",
    "ON a.id = b.id\n",
    "JOIN \n",
    "(SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags UNION ALL SELECT * from relations_tags) as c\n",
    "ON b.id = c.id\n",
    "WHERE a.key = 'amenity'\n",
    "AND b.key IN ('cuisine','diet')\n",
    "AND a.value IN ('restaurant', 'bar','cafe','pub')\n",
    "AND c.key = 'city'\n",
    "AND c.value = 'Zürich'\n",
    "GROUP BY b.value\n",
    "ORDER BY count(*) DESC \n",
    "LIMIT 21;"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "italian       | 69           american   | 7\n",
    "regional      | 47           burger     | 6\n",
    "coffee_shop   | 28           lebanese   | 6\n",
    "pizza         | 24           sushi      | 6\n",
    "asian         | 22           vegetarian | 6\n",
    "indian        | 22           greek      | 5\n",
    "international | 14           spanish    | 5\n",
    "thai          | 14           swiss      | 4\n",
    "japanese      | 10           french     | 3\n",
    "chinese       | 9            sandwich   | 3\n",
    "                             brazilian  | 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it is not really clear what kind of cuisine \"coffe_shop\" is supposed to be, I will include cuisine ranked 21st in the top 20 and ignore \"coffe_shop\".\n",
    "\n",
    "The OSM data contains information on cuisine type for 324 restaurants in Zürich in total (without \"coffe_shop\"; use the above query without grouping by cuisine value). Italian(including pizza), asian(including chinese, thai, japanese and sushi)  and regional (including swiss) cuisines are the most prominent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of restaurants, cafes, bars, pubs and top 3 cuisines per district "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SELECT a.key,b.key, a.value, b.value, count(*) \n",
    "FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags UNION ALL SELECT * from relations_tags) as a \n",
    "JOIN \n",
    "(SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags UNION ALL SELECT * from relations_tags) as b\n",
    "ON a.id = b.id\n",
    "WHERE a.key = 'amenity'\n",
    "AND b.key = 'district'\n",
    "AND a.value IN ('restaurant', 'bar','cafe','pub')\n",
    "GROUP BY b.value\n",
    "ORDER BY count(*) DESC; "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "district   count     district   count\n",
    "\n",
    "Kreis 1    150       Kreis 11   44\n",
    "Kreis 4    135       Kreis 2    37\n",
    "Kreis 5    59        Kreis 7    26\n",
    "Kreis 8    54        Kreis 9    25\n",
    "Kreis 6    46        Kreis 10   20\n",
    "Kreis 3    45        Kreis 12   7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, most food and beverage places are located in district \"Kreis 1\", which is the city center/old town"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CREATE VIEW cuisine_district as SELECT comb.dis_name,comb.cui_name,num\n",
    "FROM(\n",
    "SELECT a.key as amenity,b.key as district,c.key as cuisine, a.value as restaurant, b.value as dis_name,c.value as cui_name, count(*) as num \n",
    "FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags UNION ALL SELECT * from relations_tags) as a \n",
    "JOIN \n",
    "(SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags UNION ALL SELECT * from relations_tags) as b\n",
    "ON a.id = b.id\n",
    "JOIN\n",
    "(SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags UNION ALL SELECT * from relations_tags) as c\n",
    "ON b.id = c.id\n",
    "WHERE a.key = 'amenity'\n",
    "AND b.key = 'district'\n",
    "AND a.value IN ('restaurant', 'bar','cafe','pub')\n",
    "AND c.key IN ('cuisine','diet')\n",
    "GROUP BY b.value, c.value\n",
    "ORDER BY b.value,num DESC) as comb;\n",
    "\n",
    "\n",
    "SELECT *\n",
    "FROM cuisine_district AS a\n",
    "WHERE a.cui_name IN (\n",
    "    SELECT b.cui_name\n",
    "    FROM cuisine_district AS b\n",
    "    WHERE a.dis_name = b.dis_name\n",
    "    ORDER BY b.num DESC\n",
    "    LIMIT 4\n",
    "    )\n",
    "ORDER BY CAST(substr(dis_name,7) as INTEGER); "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "district  cuisine         count       district  cuisine         count       district   cuisine         count\n",
    "\n",
    "Kreis 1 | italian       | 10          Kreis 5 | indian        | 6           Kreis 9  | pizza         | 3\n",
    "Kreis 1 | regional      | 6           Kreis 5 | regional      | 5           Kreis 9  | regional      | 3\n",
    "Kreis 1 | asian         | 5           Kreis 5 | italian       | 4           Kreis 9  | international | 2\n",
    "Kreis 2 | italian       | 6           Kreis 6 | italian       | 6           Kreis 10 | regional      | 5\n",
    "Kreis 2 | regional      | 4           Kreis 6 | indian        | 3           Kreis 10 | indian        | 2\n",
    "Kreis 2 | pizza         | 3           Kreis 6 | pizza         | 3           Kreis 10 | british_pies  | 1\n",
    "Kreis 3 | italian       | 4           Kreis 7 | italian       | 3           Kreis 11 | italian       | 5\n",
    "Kreis 3 | chinese       | 2           Kreis 7 | regional      | 3           Kreis 11 | regional      | 5\n",
    "Kreis 3 | international | 2           Kreis 7 | american      | 1           Kreis 11 | indian        | 2\n",
    "Kreis 4 | italian       | 13          Kreis 8 | italian       | 11          Kreis 12 | bagels        | 1\n",
    "Kreis 4 | asian         | 8           Kreis 8 | french        | 2\n",
    "Kreis 4 | indian        | 7           Kreis 8 | greek         | 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, because of the ambiguous cuisine type \"coffe_shop\", I queried the top 4 cuisines, excluded \"coffe_shop\" if present in top 3, and included the cuisine ranked one position behind \"coffe_shop\". This is true for district 1, district 6 and district 11 (For the sake of clarity, manually filtered results for top 3 cuisines are shown).\n",
    "\n",
    "The data on cuisine types per district matches the ranking obtained for the city (see section above), with italian, asian or regional cuisines being the most predominant. Only in district 5 and 12, indian and bagels, respectively, are the top ranked cuisines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of facilities related to education, healthcare and public service for each district\n",
    "This includes a personal selection of \"nice to have\" nearby"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SELECT b.value, count(*) \n",
    "FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags UNION ALL SELECT * from relations_tags) as a \n",
    "JOIN \n",
    "(SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags UNION ALL SELECT * from relations_tags) as b\n",
    "ON a.id = b.id\n",
    "WHERE a.key = 'amenity'\n",
    "AND b.key = 'district'\n",
    "AND a.value IN ('school','college','library','university','kindergarten','doctors','hospital','pharmacy','clinic','dentist','bank','atm', 'car_sharing', 'bus_station')\n",
    "GROUP BY b.value\n",
    "ORDER BY count(*) DESC; "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SELECT count(*) \n",
    "FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags UNION ALL SELECT * from relations_tags) as a \n",
    "JOIN \n",
    "(SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags UNION ALL SELECT * from relations_tags) as b\n",
    "ON a.id = b.id\n",
    "WHERE a.key = 'amenity'\n",
    "AND b.key = 'city'\n",
    "AND b.value = 'Zürich'\n",
    "AND a.value IN ('school','college','library','university','kindergarten','doctors','hospital','pharmacy','clinic','dentist','bank','atm', 'car_sharing', 'bus_station')\n",
    "GROUP BY b.value\n",
    "ORDER BY count(*) DESC; "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "district   count       district  count\n",
    "\n",
    "Kreis 1  | 60          Kreis 9 | 35\n",
    "Kreis 11 | 47          Kreis 2 | 34\n",
    "Kreis 7  | 47          Kreis 3 | 31\n",
    "Kreis 6  | 46          Kreis 8 | 28\n",
    "Kreis 12 | 40          Kreis 4 | 22\n",
    "Kreis 10 | 36          Kreis 5 | 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highest and lowest counts are obtained for district 1 and 4/5, whereas similar counts are obtained for all other districts (please refere to the section Possibilities for improvements for more discussion on district-specific results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of facilities related to leisure, sports and entertainment for each district\n",
    "Again, a selection of sports activities and entertainment for which I would like to have the appropriate infrastructure nearby. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SELECT b.value, count(*) \n",
    "FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags UNION ALL SELECT * from relations_tags) as a \n",
    "JOIN \n",
    "(SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags UNION ALL SELECT * from relations_tags) as b\n",
    "ON a.id = b.id\n",
    "WHERE a.key IN ('leisure','sport','amenity')\n",
    "AND b.key = 'district'\n",
    "AND a.value IN ('cinema','theatre','park','dance','stadium','swimming_area','swimming_pool','sports_centre','basketball','beachvolleyball','soccer','swimming','cycling')\n",
    "GROUP BY b.value\n",
    "ORDER BY count(*) DESC; "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "district   count     district   count\n",
    "\n",
    "Kreis 1  | 18        Kreis 4  | 4\n",
    "Kreis 6  | 9         Kreis 5  | 4\n",
    "Kreis 9  | 8         Kreis 8  | 4\n",
    "Kreis 3  | 6         Kreis 2  | 3\n",
    "Kreis 11 | 4         Kreis 7  | 3\n",
    "Kreis 12 | 4         Kreis 10 | 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "District 1 has the highest number of relevant facilitiest, whereas all other districts exibit rather lower counts (please refere to the section Possibilities for improvements for more discussion on district-specific results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of shops for each district\n",
    "This is a selection of shops that I would consider nice to have nearby. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SELECT b.value, count(*) \n",
    "FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags UNION ALL SELECT * from relations_tags) as a \n",
    "JOIN \n",
    "(SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags UNION ALL SELECT * from relations_tags) as b\n",
    "ON a.id = b.id\n",
    "WHERE a.key = 'shop'\n",
    "AND b.key = 'district'\n",
    "AND a.value IN ('supermarket','bakery','convenience','butcher','shoes','books','chemist','deli','greengrocer','pastry' )\n",
    "GROUP BY b.value\n",
    "ORDER BY count(*) DESC; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "district   count     district   count\n",
    "\n",
    "Kreis 1  | 38        Kreis 5  | 19\n",
    "Kreis 11 | 38        Kreis 3  | 18\n",
    "Kreis 2  | 30        Kreis 8  | 18\n",
    "Kreis 9  | 28        Kreis 10 | 16\n",
    "Kreis 6  | 23        Kreis 7  | 11\n",
    "Kreis 4  | 20        Kreis 12 | 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, more or less similar counts were obtained for all districts but for district 7 and 12. The lower counts are probably due to missing  information in the OSM dataset and don't represent true numbers for shops (please refer to the section Possibilities for improvements for more discussion on district-specific results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possibilities for improvement of the dataset or improvement of the analysis \n",
    "**increasing the number of contributions and quantity of relevant information**  \n",
    "Only for 324 of the total 736 entries depicting food and beverage localities in Zurich, the relevant information on the cuisine type is present in the dataset (534 total entries and 317 including cuisine, if restricted to only restaurant; queries not shown).  \n",
    "Maybe the restaurants themselves can contribute to encourage the user community to participate in updating the OSM dataset. I am thinking of restaurants offering voucher for meals/drinks to anyone who makes updates on the dataset that would provide the most relevant information about the cognate restaurant (street name, postcode, district and quarter affiliation, opening hours, cuisine, average price for meals, etc.). This would be attractive especially for people living nearby a restaurant (or at least in the same quarter/district), as the benefit of getting free meals/discounts could prevail the effort for conducting an update (assuming basic computer knowledge). The restaurants could try to stimulate overall contributions by offering really valuable benefits (expensive meals, meals for two people, etc.) as the overall costs would still be very low, in contrast to extremely high marketing value.  \n",
    "\n",
    "In general, there is a discrepancy between the total counts for particular map features for the city of Zürich and the total counts for the same map feature but including information on district affiliation. For example, there are 263 \"shops\" that also contain information on district affiliation (use query in the section above without grouping), whereas a total of 1199 \"shops\" in Zürich are present in the dataset (see code below). So either the relevant XML elements have no street tags at all, or the combination of street tags/city tags and street tags/postcodes tags, respectively, don't match the reference dataset, which in both cases prevents to include information on district affiliation (for more details see section \"Strategy for correcting street names, city name and postcodes\")."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SELECT b.value, count(*) \n",
    "FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags UNION ALL SELECT * from relations_tags) as a \n",
    "JOIN \n",
    "(SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags UNION ALL SELECT * from relations_tags) as b\n",
    "ON a.id = b.id\n",
    "WHERE a.key = 'shop'\n",
    "AND b.key = 'city'\n",
    "AND b.value = 'Zürich';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the local government could support a general benefit/voucher-based system in context with creating/updating OSM data, maybe even in cooperation with regional facilities (e.g cinemas, etc.), e.g for every 50-100 updates, you would get a discount in the local cinema/theatre. Again, this might have a big marketing impact without creating too high costs for such a program. \n",
    "\n",
    "The main issue would be to ensure that user contribution is somehow officially verifiable (e.g, proof for eligibility for any benefits/discounts) and that concomitantly the manipulation of OSM data matches certain quality standards (you don't want users to simply do nonsense contributions to be eligible for benefits/discounts). For the second point, some kind of \"audit automation (maybe similar scripts as used in this project)\" that automatically screen every new entry would be required in the backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**improving the accuracy of contributions**  \n",
    "Only 45% of all city tags actually correspond to the city of Zürich (see query below)!!! So the majority of entries including information about city affiliation actually depict Zurich municipality. The reason, on the one hand, is that the Export function used to download the XML file from the openstreetmap homepage is using wrong default values for the city boundaries of Zürich city (minlat='47.2734000' maxlon='8.8031000' minlon='8.2700000' maxlat='47.4815000'). Thats an important detail one cannot expect to be aware of **\"You don't know what you don't know\"**. I checked the border coordinates manually, only because of the large number of wrong city tags and postcode tags exposed from auditing the data. The approximate correct city coordinates are: minlat='47.3381', maxlon='8.6270', minlon='8.4481', maxlat='47.4351'."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SELECT ROUND(AVG(CASE WHEN a.value = 'Zürich' then 1.0 ELSE 0 END)*100,2)\n",
    "FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags UNION ALL SELECT * from relations_tags) as a \n",
    "WHERE a.key = 'city'\n",
    "AND a.type = 'addr';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are 1055 nodes/tags that specify city as Zürich despite coordinates outside the boundaries of Zürich city (see query below). So I am thinking of constraints on coordinates -at least for major cities- in the backend. Each time user violate these predefined constraints with a wrong entry, the system should return a warning message stating that the respective coordinates might be wrong. On the other hand, including such constraints in data auditing, would be easy to implement in the underlying analysis. However, there should be at least an explicit warning message on ambiguous coordinates when downloading XML data from openstreetmap to remind users."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SELECT count(*)\n",
    "FROM \n",
    "nodes_tags JOIN nodes\n",
    "ON nodes_tags.id = nodes.id\n",
    "WHERE key = 'city'\n",
    "AND type = 'addr'\n",
    "AND value = 'Zürich'\n",
    "AND (lon < 8.4481 OR lon > 8.627 OR lat < 47.3381 OR lat > 47.4351);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**improving quality of contributions**  \n",
    "Dialect used for naming is present in the dataset (see query and result below) and this is especially a problem if you want to include a reference dataset for auditing/cleaning (for more information see auditing street names and strategy for correcting street names). Either you will fail to match with the reference data (\"Chlösterlistrasse\" in dialect refers actually to \"Klosterstrasse\") or some names might even be falsely identified as mistake-containing entries (like for the street type \"gasse\", which is written \"gass\" in dialect).\n",
    "Actually, this problem can only be tackled by raising the awareness of users. Maybe, for each second level tag a mandatory key:value pair could be introduced that requires the user to specify whether the official language or a dialect has been used. This would still allow to use either of both, however, for third parties conducting data auditing/cleaning, this \"language-version\" could be of much help.  \n",
    "Nevertheless, the big problem is that no objective control could be installed for what is actually assigned to the aforementioned key:value pairs. Consequently, users could make wrong specifications on purpose (e.g, specifying official language when using dialect instead)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SELECT DISTINCT(value)\n",
    "FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags UNION ALL SELECT * from relations_tags)\n",
    "WHERE key = 'street' \n",
    "AND value LIKE '%gass%'\n",
    "AND value NOT LIKE '%gasse%'\n",
    "OR value LIKE '%Chlöster%';"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Bike Shop Chlösterli        Chlösterlistrasse\n",
    "Chriesigass                 Chrattengass\n",
    "Foregass                    Chilegass\n",
    "Im Chlösterli               Büelgass\n",
    "Gassackerstrasse            Chlösterliweg\n",
    "Chlösterli-Weg              Gassäckerweg\n",
    "Im Gassacher                Stedtligass\n",
    "Gassacherstrasse            Gubelgass\n",
    "Gassäckerstrasse            Hohlgass\n",
    "Schmittegass                Chüegass\n",
    "                            Steigass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Screening for mistakes, , abbreviations, language variants in street names turned out to be more difficult than expected. The reason is that german street names don't follow a distinct naming pattern, including street types as independent words within the name. Instead a large number of streets has arbitrary names. Especially with growing size of data, the number of false negatives is increasing, which at some point will compromise the developed strategy, because false negatives will prevail.  \n",
    "The other major issue was the presence of data points that actually don't belong to the selected dataset (here, the city Zurich in Switzerland). However, once being aware of this, one can easily implement a strategy to either select and mark those data points, or even exclude them from the final database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References used for the project\n",
    "\n",
    "General Information on OSM data  \n",
    "http://wiki.openstreetmap.org/wiki\n",
    "\n",
    "Source to mine data for setting up the reference dataset for Zurich city  \n",
    "http://www.streetdir.ch/CH/Zurich/Zurich/Strassenverzeichnis\n",
    "\n",
    "Information about expected combinations of districts and quarters, as well as quarters and postcodes  \n",
    "http://www.plz-suche.org/zuerich-ch7e45, \n",
    "https://en.wikipedia.org/wiki/Subdivisions_of_Z%C3%BCrich"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python2]",
   "language": "python",
   "name": "conda-env-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
